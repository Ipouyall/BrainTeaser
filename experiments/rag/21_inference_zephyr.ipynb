{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_PATH = \"./SE2024\"\n",
    "INFERENCE_DATASET_PATH = f\"{BASE_PATH}/test_split_data.csv\"\n",
    "SHOTS_DATASET_PATH = f\"{BASE_PATH}/shots_data.jsonl\"\n",
    "OUTPUT_DATA_PATH = f\"{BASE_PATH}/inference_data.jsonl\"\n",
    "OUTPUT_BACKUP_PATH = f\"{BASE_PATH}/inference_data_backup.jsonl\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf3bebb6eef07ac6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(BASE_PATH, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a42ea339b0c6450a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74c6f12665c17c7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl(address):\n",
    "    json_list = []\n",
    "    with open(address, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            json_list.append(data)\n",
    "    return json_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38844b73cf2fd911"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def handle_missing_data(data_path, data_id):\n",
    "    if os.path.exists(data_path):\n",
    "        print('Data file already exists')\n",
    "    else:\n",
    "        print(\"Data doesn't exist, start download from the google drive...\")\n",
    "        !gdown $data_id -O $data_path"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e71807284ece3a76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "handle_missing_data(INFERENCE_DATASET_PATH, \"1JcpBjTXv2OfaG6uYcIJO-Yk69nT9uN8i\")\n",
    "inference_data = pd.read_csv(INFERENCE_DATASET_PATH)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d6cebbe24238b29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "handle_missing_data(SHOTS_DATASET_PATH, \"1Byb7jvUfdmIWt39Wkmr69ygmUlGrEHEJ\") # 11-simple-relation\n",
    "# handle_missing_data(SHOTS_DATASET_PATH, \"1mxzecUyXIbE891E2m5mvotLC4k7VSaUH\") # 12-ranker-relation\n",
    "# handle_missing_data(SHOTS_DATASET_PATH, \"1skMfKvlN7pTKdqIKr16IZV0tawDaTGz_\") # 11-simple-summarize\n",
    "# handle_missing_data(SHOTS_DATASET_PATH, \"1pGVJKDQSjqHpwqsivHonWneE918jXt31\") # 12-ranker-summarize\n",
    "shots_data = read_jsonl(SHOTS_DATASET_PATH)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aa88465af32fbdd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Template"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97f8993d5587969"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\\n",
    "<|Instruction|>\n",
    "Decode a riddle from four options using everyday logic and creativity. \\\n",
    "Riddles may involve misdirection, double meanings, metaphorical wordplay, exaggeration, common phrases, associations, irony, numerical puzzles, and elemental imagery. \\\n",
    "Avoid gender bias. \\\n",
    "Keep solutions within realistic imagination and avoid supernatural elements.\n",
    "I would provide you similar Riddle-Answer pair and the logic behind them.\n",
    "Lets think step by step for each of the options and at the end, \\\n",
    "provide the best option in the format 'Option 1,' 'Option 2,' 'Option 3,' or 'Option 4.'\n",
    "\n",
    "<|Samples|>\n",
    "{examples}\n",
    "\n",
    "<|Problem|>\n",
    "Riddle: \"{riddle}\"\n",
    "Option 1: \"{option_1}\"\n",
    "Option 2: \"{option_2}\"\n",
    "Option 3: \"{option_3}\"\n",
    "Option 4: \"None of the above options are correct\"\n",
    "\n",
    "Let's think step by step about each option, then at the end, choose the best and the most logical option:\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(question, samples):\n",
    "    examples = \"\"\n",
    "    for k,v in samples.items():\n",
    "        if \"shot\" in k:\n",
    "            examples += v + \"\\n\"\n",
    "    return prompt_template.format(\n",
    "        riddle=question['QUESTION'],\n",
    "        option_1=question['OPTION 1'],\n",
    "        option_2=question['OPTION 2'],\n",
    "        option_3=question['OPTION 3'],\n",
    "        examples=examples\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba97cd3937c89f62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare LLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc6bcd149b7e3ace"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes transformers accelerate torch\n",
    "!pip install -q safetensors xformers langchain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b9097e958a4e749"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e27e5606d60547f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f8b47cc346d4291"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_cache=True,\n",
    "        device_map=\"auto\",\n",
    "        max_length=4000,\n",
    "        do_sample=True,\n",
    "        top_k=5,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88fc72bb590a763"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature': 0.0})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62244a66e66adf13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference Utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15a355ddeb652b1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_inference(data, address):\n",
    "    with open(address, 'w') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')\n",
    "            \n",
    "def add_inference(data, address):\n",
    "    with open(address, 'a+') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a45e22965a3d6bfa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execute Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f8e9a165d206798"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "results = []\n",
    "itr = tqdm(zip(inference_data.iterrows(), shots_data), total=len(inference_data), desc=\"Processing\")\n",
    "\n",
    "for (index, ds), shot in itr:\n",
    "    prompt = get_prompt(ds, shot)\n",
    "    result = llm.invoke(prompt)\n",
    "\n",
    "    data = {\n",
    "        \"question\": ds['QUESTION'],\n",
    "        'option 1': ds['OPTION 1'],\n",
    "        'option 2': ds['OPTION 2'],\n",
    "        'option 3': ds['OPTION 3'],\n",
    "        'option 4': ds['OPTION 4'],\n",
    "        'zephyr': result\n",
    "    }\n",
    "    add_inference([data], OUTPUT_DATA_PATH)\n",
    "    results.append(data)\n",
    "    \n",
    "save_inference(results, OUTPUT_BACKUP_PATH)\n",
    "\n",
    "print(f\"Dumped {len(results)} records to {OUTPUT_DATA_PATH}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b37c0d2a3202a564"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
