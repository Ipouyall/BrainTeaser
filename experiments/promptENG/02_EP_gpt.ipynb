{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "!pip install -q -U openai\n",
    "!pip install -q gdown==v4.6.3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:55:47.580846Z",
     "start_time": "2024-01-24T15:55:27.449924Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "import datetime\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:55:47.957540Z",
     "start_time": "2024-01-24T15:55:47.580356Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-4\"\n",
    "DATA_DIR = 'SE2024'\n",
    "INPUT_DATA_PATH  = f'{DATA_DIR}/test_split.csv'\n",
    "OUTPUT_DATA_PATH = f'{DATA_DIR}/infer.jsonl'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:55:47.966488Z",
     "start_time": "2024-01-24T15:55:47.958517Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def gdrive_download(file_id, file_name):\n",
    "    !gdown $file_id --output $file_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:55:47.966655Z",
     "start_time": "2024-01-24T15:55:47.960987Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=1JcpBjTXv2OfaG6uYcIJO-Yk69nT9uN8i\r\n",
      "To: /Users/pooyasadeghi/PycharmProjects/BrainTeaser/experiments/promptENG/SE2024/test_split\r\n",
      "100%|███████████████████████████████████████| 39.0k/39.0k [00:00<00:00, 439kB/s]\r\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "if not os.path.exists(INPUT_DATA_PATH):\n",
    "    gdrive_download('1JcpBjTXv2OfaG6uYcIJO-Yk69nT9uN8i', INPUT_DATA_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:55:50.754060Z",
     "start_time": "2024-01-24T15:55:47.964792Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(INPUT_DATA_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:55:50.764030Z",
     "start_time": "2024-01-24T15:55:50.755318Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chat Bots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import openai\n",
    "OPENAI_API_KEY = \"...\"\n",
    "\n",
    "class GPTBot:\n",
    "    def __init__(self, model=\"gpt-4\"):\n",
    "        print(\"Initiating GPT chat bot...\")\n",
    "        \n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "        self.model = model\n",
    "        print(\"GPT chat bot Initiated!\")\n",
    "\n",
    "    def get_completion(self, prompt):\n",
    "        i = 0\n",
    "        while i<10:\n",
    "            i+=1\n",
    "            try:\n",
    "                completion = self.__get_completion_handler(prompt)\n",
    "            except:\n",
    "                print(f\"GPT completion failed ::[{datetime.datetime.now()}]::\")\n",
    "                time.sleep(10)\n",
    "                print(f\"Trying GPT completion ::[{datetime.datetime.now()}]::\")\n",
    "            else:\n",
    "                break\n",
    "        return completion\n",
    "\n",
    "    def __get_completion_handler(self, prompt):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0, # this is the degree of randomness of the model's output\n",
    "        )\n",
    "        return response.choices[0].message[\"content\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:58:28.720874Z",
     "start_time": "2024-01-24T15:58:28.701866Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Setup"
   ],
   "metadata": {
    "id": "ZOn1CfT8BFQ4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "base_prompt = \"\"\"\n",
    "You are given a riddle and four options to choose the answer amongst them. A riddle is a question or statement intentionally phrased so as to require ingenuity in ascertaining its answer or meaning, typically presented as a game.\n",
    "Different ideas can be used in these riddles:\n",
    "    1. Riddles often employ misdirection, leading you away from the actual solution.\n",
    "    2. They include elements with double meanings, requiring a keen eye for words with dual interpretations.\n",
    "    3. Metaphorical wordplay adds another layer, urging you to decipher figurative language.\n",
    "    4. Look out for exaggeration, as riddles may present overly dramatic details to divert your attention.\n",
    "    5. Common phrases and sayings may hide within the puzzle, demanding familiarity.\n",
    "    6. Associations and irony play a crucial role, introducing unexpected connections.\n",
    "    7. Numerical puzzles can also be part of the mystery, requiring you to decode their significance.\n",
    "    8. Elemental imagery, drawn from nature, might hold key descriptors.\n",
    "    9. Rhyming and sound clues can add a poetic dimension.\n",
    "    10. Avoid sexism ans sex cliche, for example, gender bias for jobs, based on their positions or their outcome.\n",
    "    11. Also, it is important to note you should decode the upcoming riddle using everyday logic and creativity.\n",
    "Although a clever solution is required, avoid supernatural solutions and keep your answer within the limits of realistic imagination. For example, having superhuman abilities or unusual events or things are mostly a not preferred choice unless that is a better solution. Now which of the following options is the answer to the following riddle:\n",
    "\n",
    "to solve the riddle, consider mentioned pont above and think step by step for each option, and it the end, mention the option you think is the best, in the format: 'Option 1' or 'Option 2' or 'Option 3' or 'Option 4'\n",
    "\n",
    "\n",
    "Riddle: \"{RIDDLE}\"\n",
    "\n",
    "Options:\n",
    "Option 1: \"{OPTION_1}\"\n",
    "Option 2: \"{OPTION_2}\"\n",
    "Option 3: \"{OPTION_3}\"\n",
    "Option 4: \"None of the above options are correct\"\n",
    "\n",
    "\n",
    "Let's think step by step about each option, then at the end, choose the best and the most logical option:\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(ds):\n",
    "    return base_prompt.format(\n",
    "            RIDDLE=ds['QUESTION'],\n",
    "            OPTION_1=ds['OPTION 1'],\n",
    "            OPTION_2=ds['OPTION 2'],\n",
    "            OPTION_3=ds['OPTION 3'],\n",
    "    )\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQENRkzmA0An",
    "outputId": "3a6fdec3-0f68-48a9-a233-5e9d97d4e5ae",
    "ExecuteTime": {
     "end_time": "2024-01-24T15:55:51.132862Z",
     "start_time": "2024-01-24T15:55:51.118712Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read/Write utils"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def save_inference(data, address):\n",
    "    with open(address, 'w') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:55:51.132981Z",
     "start_time": "2024-01-24T15:55:51.121035Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def add_inference(data, address):\n",
    "    with open(address, 'a+') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:55:51.133065Z",
     "start_time": "2024-01-24T15:55:51.123032Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def read_inference(address):\n",
    "    json_list = []\n",
    "    with open(address, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            json_list.append(data)\n",
    "    return json_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:55:51.133518Z",
     "start_time": "2024-01-24T15:55:51.125034Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "if os.path.exists(OUTPUT_DATA_PATH):\n",
    "    results = read_inference(OUTPUT_DATA_PATH)\n",
    "\n",
    "    print(f\"Records recovered from {OUTPUT_DATA_PATH}\")\n",
    "    print(f\".::{len(results)} records::.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:55:51.134505Z",
     "start_time": "2024-01-24T15:55:51.127149Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating GPT chat bot...\n",
      "GPT chat bot Initiated!\n"
     ]
    }
   ],
   "source": [
    "chat_bot = GPTBot(model=GPT_MODEL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:58:37.970588Z",
     "start_time": "2024-01-24T15:58:37.951092Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execute experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/120 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1a2723461aa465eba2eacaf969cd231"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT completion failed ::[2024-01-24 19:28:41.866417]::\n",
      "Trying GPT completion ::[2024-01-24 19:28:51.871628]::\n",
      "GPT completion failed ::[2024-01-24 19:28:51.872306]::\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAPIRemovedInV1\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 18\u001B[0m, in \u001B[0;36mGPTBot.get_completion\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 18\u001B[0m     completion \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_completion_handler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "Cell \u001B[0;32mIn[17], line 29\u001B[0m, in \u001B[0;36mGPTBot.__get_completion_handler\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m     28\u001B[0m messages \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: prompt}]\n\u001B[0;32m---> 29\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# this is the degree of randomness of the model's output\u001B[39;49;00m\n\u001B[1;32m     33\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39mchoices[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmessage[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/BrainTeaser/venvv/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001B[0m, in \u001B[0;36mAPIRemovedInV1Proxy.__call__\u001B[0;34m(self, *_args, **_kwargs)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m_args: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_kwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m---> 39\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m APIRemovedInV1(symbol\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_symbol)\n",
      "\u001B[0;31mAPIRemovedInV1\u001B[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m      7\u001B[0m prompt \u001B[38;5;241m=\u001B[39m get_prompt(ds)\n\u001B[0;32m----> 8\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mchat_bot\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_completion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m data \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m: ds[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQUESTION\u001B[39m\u001B[38;5;124m'\u001B[39m], \n\u001B[1;32m     12\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moption 1\u001B[39m\u001B[38;5;124m'\u001B[39m: ds[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOPTION 1\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpt\u001B[39m\u001B[38;5;124m'\u001B[39m: result\n\u001B[1;32m     17\u001B[0m }\n\u001B[1;32m     18\u001B[0m add_inference([data], OUTPUT_DATA_PATH)\n",
      "Cell \u001B[0;32mIn[17], line 21\u001B[0m, in \u001B[0;36mGPTBot.get_completion\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPT completion failed ::[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdatetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]::\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrying GPT completion ::[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdatetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]::\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = len(results)\n",
    "\n",
    "for idx, (index, ds) in enumerate(tqdm(dataset.iterrows(), total=len(dataset)), start=1):\n",
    "    if idx <= start:\n",
    "        continue\n",
    "\n",
    "    prompt = get_prompt(ds)\n",
    "    result = chat_bot.get_completion(prompt)\n",
    "    \n",
    "    data = {\n",
    "        \"question\": ds['QUESTION'], \n",
    "        'option 1': ds['OPTION 1'],\n",
    "        'option 2': ds['OPTION 2'],\n",
    "        'option 3': ds['OPTION 3'],\n",
    "        'option 4': ds['OPTION 4'],\n",
    "        'gpt': result\n",
    "    }\n",
    "    add_inference([data], OUTPUT_DATA_PATH)\n",
    "save_inference(results, OUTPUT_DATA_PATH)\n",
    "\n",
    "print(f\"Dumped {len(results)} records to {OUTPUT_DATA_PATH}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T15:58:56.343001Z",
     "start_time": "2024-01-24T15:58:41.867194Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
