{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_PATH = \"./SE2024\"\n",
    "INFERENCE_DATASET_PATH = f\"{BASE_PATH}/test_split_data.csv\"\n",
    "\n",
    "OUTPUT_DATA_PATH = f\"{BASE_PATH}/inference_data.jsonl\"\n",
    "OUTPUT_BACKUP_PATH = f\"{BASE_PATH}/inference_data_backup.jsonl\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf3bebb6eef07ac6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(BASE_PATH, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a42ea339b0c6450a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74c6f12665c17c7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def handle_missing_data(data_path, data_id):\n",
    "    if os.path.exists(data_path):\n",
    "        print('Data file already exists')\n",
    "    else:\n",
    "        print(\"Data doesn't exist, start download from the google drive...\")\n",
    "        !gdown $data_id -O $data_path"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e71807284ece3a76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "handle_missing_data(INFERENCE_DATASET_PATH, \"1JcpBjTXv2OfaG6uYcIJO-Yk69nT9uN8i\")\n",
    "inference_data = pd.read_csv(INFERENCE_DATASET_PATH)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d6cebbe24238b29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Template"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97f8993d5587969"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\\n",
    "Your task is to generate a descriptive explanation from a question to an answer option. \\\n",
    "In the following, a question and an option as the answer to the question is provided. \\\n",
    "The answer might be or not be a correct answer. \\\n",
    "Write a descriptive explanation in at most one paragraph and 200 words to show that path from question to the answer.\n",
    "\n",
    "Question: \"{question}\"\n",
    "Answer Option: \"{option}\"\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(question):\n",
    "    return [prompt_template.format(question=question['QUESTION'], option=question['OPTION 1']),\n",
    "            prompt_template.format(question=question['QUESTION'], option=question['OPTION 2']),\n",
    "            prompt_template.format(question=question['QUESTION'], option=question['OPTION 3'])]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba97cd3937c89f62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decision_template = \"\"\" \\\n",
    "You are given a riddle and four options to choose the answer amongst them. \\\n",
    "The fourth option is \"None of the above options\". \\\n",
    "Your final task is choosing the best option that is related to the riddle. \\\n",
    "For the first three options, you are given a context that explains a path between the question and the answer. \\\n",
    "Although these contexts may try to say their option is true, you should compare all the options based on the question \\\n",
    "and options' context to choose the one that has the most logical answer. If none of them seem logical, \n",
    "choose the fourth option: \"None of the above options.\" \\\n",
    "Now, consider the riddle below and the context provided for you, and tell me which option is \\\n",
    "the best answer to the riddle due to the context. \\\n",
    "\n",
    "Riddle: \"{question}\"\n",
    "\n",
    "Options:\n",
    "Option 1: \"{option 1}\"\n",
    "Option 2: \"{option 2}\"\n",
    "Option 3: \"{option 3}\"\n",
    "Option 4: \"None of the above options.\"\n",
    "\n",
    "Contexts:\n",
    "Context about option 1: \"{context 1}\"\n",
    "Context about option 2: \"{context 2}\"\n",
    "Context about option 3: \"{context 3}\"\n",
    "\n",
    "To answer this riddle, you should exactly mention one option, \\\n",
    "so announce the option you think is the best one in the format: 'Option 1' or 'Option 2' or 'Option 3' or 'Option 4':\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "564ecf09d36a302e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_answer(result: str):\n",
    "    o1 = result.rfind(\"Option 1\")\n",
    "    o2 = result.rfind(\"Option 2\")\n",
    "    o3 = result.rfind(\"Option 3\")\n",
    "    o4 = result.rfind(\"Option 4\")\n",
    "\n",
    "    answer = np.argmax([o1, o2, o3, o4])\n",
    "    return answer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e074aea86162f5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare LLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc6bcd149b7e3ace"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate torch\n",
    "!pip install -q safetensors xformers langchain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b9097e958a4e749"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e27e5606d60547f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f8b47cc346d4291"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_cache=True,\n",
    "        device_map=\"auto\",\n",
    "        max_length=4000,\n",
    "        do_sample=True,\n",
    "        top_k=5,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88fc72bb590a763"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature': 0.0})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62244a66e66adf13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference Utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15a355ddeb652b1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_inference(data, address):\n",
    "    with open(address, 'w') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')\n",
    "            \n",
    "def add_inference(data, address):\n",
    "    with open(address, 'a+') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a45e22965a3d6bfa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execute Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f8e9a165d206798"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "results = []\n",
    "itr = tqdm(inference_data.iterrows(), total=len(inference_data), desc=\"Processing\")\n",
    "\n",
    "for index, ds in itr:\n",
    "    data = {\n",
    "        \"question\": ds['QUESTION'],\n",
    "        'option 1': ds['OPTION 1'],\n",
    "        'option 2': ds['OPTION 2'],\n",
    "        'option 3': ds['OPTION 3'],\n",
    "    }\n",
    "    context_prompts = get_prompt(ds)\n",
    "    for i, context_prompt in enumerate(context_prompts, start=1):\n",
    "        data[f'context {i}'] = llm.invoke(context_prompt).strip()\n",
    "    result_prompt = decision_template.format(**data)\n",
    "    data['option 4'] = ds['OPTION 4']\n",
    "    result = llm.invoke(result_prompt).strip()\n",
    "    data['zephyr'] = extract_answer(result)\n",
    "    \n",
    "    add_inference([data], OUTPUT_DATA_PATH)\n",
    "    results.append(data)\n",
    "    \n",
    "save_inference(results, OUTPUT_BACKUP_PATH)\n",
    "\n",
    "print(f\"Dumped {len(results)} records to {OUTPUT_DATA_PATH}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b37c0d2a3202a564"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
