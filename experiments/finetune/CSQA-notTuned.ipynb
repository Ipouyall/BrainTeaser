{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q gdown datasets==2.17.0\n",
    "!pip install -q transformers\n",
    "print(\"transformers\")\n",
    "!pip install -q accelerate\n",
    "print(\"accelerate\")\n",
    "!pip install -q torch\n",
    "print(\"torch\")\n",
    "!pip install -q safetensors\n",
    "print(\"safetensors\")\n",
    "!pip install -q xformers\n",
    "print(\"xformers\")\n",
    "!pip install -q langchain==0.1.6\n",
    "print(\"langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_PATH = \"./SE2024/csqa\"\n",
    "\n",
    "OUTPUT_DATA_PATH = f\"{BASE_PATH}/inference_data.jsonl\"\n",
    "OUTPUT_BACKUP_PATH = f\"{BASE_PATH}/inference_data_backup.jsonl\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf3bebb6eef07ac6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(BASE_PATH, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a42ea339b0c6450a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74c6f12665c17c7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e71807284ece3a76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = load_dataset(\"tau/commonsense_qa\", split='validation')[:150]\n",
    "itr = zip(df['question'], df['choices']['text'], df['answerKey'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d6cebbe24238b29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Template"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97f8993d5587969"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate,  LLMChain\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "template = \"\"\" \\\n",
    "I would provide you a question and five options. \\\n",
    "The question is designed in common sense reasoning evaluation format and to answer question, \\\n",
    "you need to choose the best option that is related to the question and is logical. \\\n",
    "You may need to think of the problem from another perspective to find the best answer.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Option 1: \"{option_1}\"\n",
    "Option 2: \"{option_2}\"\n",
    "Option 3: \"{option_3}\"\n",
    "Option 4: \"{option_4}\"\n",
    "Option 5: \"{option_5}\"\n",
    "\n",
    "o answer this question, you should exactly mention one option, \\\n",
    "so announce the option you think is the best one in the format: \\\n",
    "'Option 1' or 'Option 2' or 'Option 3' or 'Option 4' or 'Option 5':\n",
    "\"\"\"\n",
    "template = template.strip()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\", \"option_1\", \"option_2\", \"option_3\", \"option_4\", \"option_5\"],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba97cd3937c89f62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decision_template = \"\"\" \\\n",
    "You are given a riddle and four options to choose the answer amongst them. \\\n",
    "The fourth option is \"None of the above options\". \\\n",
    "Your final task is choosing the best option that is related to the riddle. \\\n",
    "For the first three options, you are given a context that explains a path between the question and the answer. \\\n",
    "Although these contexts may try to say their option is true, you should compare all the options based on the question \\\n",
    "and options' context to choose the one that has the most logical answer. If none of them seem logical, \n",
    "choose the fourth option: \"None of the above options.\" \\\n",
    "Now, consider the riddle below and the context provided for you, and tell me which option is \\\n",
    "the best answer to the riddle due to the context. \\\n",
    "\n",
    "Riddle: \"{question}\"\n",
    "\n",
    "Options:\n",
    "Option 1: \"{option 1}\"\n",
    "Option 2: \"{option 2}\"\n",
    "Option 3: \"{option 3}\"\n",
    "Option 4: \"None of the above options.\"\n",
    "\n",
    "Contexts:\n",
    "Context about option 1: \"{context 1}\"\n",
    "Context about option 2: \"{context 2}\"\n",
    "Context about option 3: \"{context 3}\"\n",
    "\n",
    "To answer this riddle, you should exactly mention one option, \\\n",
    "so announce the option you think is the best one in the format: 'Option 1' or 'Option 2' or 'Option 3' or 'Option 4':\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "564ecf09d36a302e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_csqa_answer(result: str):\n",
    "    o1 = result.rfind(\"Option 1\")\n",
    "    o2 = result.rfind(\"Option 2\")\n",
    "    o3 = result.rfind(\"Option 3\")\n",
    "    o4 = result.rfind(\"Option 4\")\n",
    "    o5 = result.rfind(\"Option 5\")\n",
    "\n",
    "    answer = np.argmax([o1, o2, o3, o4, o5])\n",
    "    option_to_answer = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\", 4: \"E\"}\n",
    "    return option_to_answer[answer], answer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e074aea86162f5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare LLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc6bcd149b7e3ace"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e27e5606d60547f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f8b47cc346d4291"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_cache=True,\n",
    "        device_map=\"auto\",\n",
    "        max_length=4000,\n",
    "        do_sample=True,\n",
    "        top_k=5,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88fc72bb590a763"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature': 0.0})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62244a66e66adf13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54551fdcee0b6077"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference Utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15a355ddeb652b1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_inference(data, address):\n",
    "    with open(address, 'w') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')\n",
    "            \n",
    "def add_inference(data, address):\n",
    "    with open(address, 'a+') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a45e22965a3d6bfa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execute Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f8e9a165d206798"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "results = []\n",
    "score = 0\n",
    "\n",
    "for que, options, key in tqdm(itr, total=150, desc=\"Inference (CSQA)\"):\n",
    "    data = {\"question\": que}\n",
    "    for i, opt in enumerate(options, start=1):\n",
    "        data[f\"option_{i}\"] = opt\n",
    "    result = llm_chain.run(data).strip()\n",
    "    data[\"zephyr_raw\"] = result\n",
    "    pred_key, pred = extract_csqa_answer(result)\n",
    "    data[\"zephyr_pred\"] = str(pred)\n",
    "    data[\"zephyr_pred_key\"] = pred_key\n",
    "    data[\"answer\"] = str(key)\n",
    "    data[\"score\"] = pred_key == key\n",
    "    add_inference([data], OUTPUT_DATA_PATH)\n",
    "    results.append(data)\n",
    "    if pred_key == key:\n",
    "        score += 1\n",
    "        print(f\"CSQA Score: {round(score/1.5, 3)}%\")\n",
    "        \n",
    "save_inference(results, OUTPUT_BACKUP_PATH)\n",
    "\n",
    "print(f\"Dumped {len(results)} records to {OUTPUT_DATA_PATH}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b37c0d2a3202a564"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
