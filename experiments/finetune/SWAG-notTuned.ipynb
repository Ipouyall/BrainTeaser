{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q gdown datasets\n",
    "!pip install -q transformers\n",
    "print(\"transformers\")\n",
    "!pip install -q accelerate\n",
    "print(\"accelerate\")\n",
    "!pip install -q torch\n",
    "print(\"torch\")\n",
    "!pip install -q safetensors\n",
    "print(\"safetensors\")\n",
    "!pip install -q xformers\n",
    "print(\"xformers\")\n",
    "!pip install -q langchain==0.1.6\n",
    "print(\"langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_PATH = \"./SE2024/swag\"\n",
    "\n",
    "OUTPUT_DATA_PATH = f\"{BASE_PATH}/inference_data.jsonl\"\n",
    "OUTPUT_BACKUP_PATH = f\"{BASE_PATH}/inference_data_backup.jsonl\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf3bebb6eef07ac6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(BASE_PATH, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a42ea339b0c6450a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74c6f12665c17c7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e71807284ece3a76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = load_dataset(\"swag\", split='validation')[:150]\n",
    "itr = zip(\n",
    "    df['startphrase'],\n",
    "    df['ending0'], df['ending1'], df['ending2'], df['ending3'],\n",
    "    df['label'],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d6cebbe24238b29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Template"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97f8993d5587969"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate,  LLMChain\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "template = \"\"\" \\\n",
    "I would provide you a question and four options. \\\n",
    "The question is designed in common sense reasoning evaluation im completion style. \\\n",
    "Give a prompt and four options, \\\n",
    "you should find the option that could complete the prompt in commonsensical and logical way and being related to prompt. \\\n",
    "You may need to think of the problem from another perspective to find the best answer.\n",
    "\n",
    "Prompt: \"{question} ...\"\n",
    "\n",
    "Option 1: \"{option_1}\"\n",
    "Option 2: \"{option_2}\"\n",
    "Option 3: \"{option_3}\"\n",
    "Option 4: \"{option_4}\"\n",
    "\n",
    "o answer this question, you should exactly mention one option, \\\n",
    "so announce the option you think is the best one in the format: \\\n",
    "'Option 1' or 'Option 2' or 'Option 3' or 'Option 4':\n",
    "\"\"\"\n",
    "template = template.strip()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\", \"option_1\", \"option_2\", \"option_3\", \"option_4\"],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba97cd3937c89f62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_swag_answer(result: str):\n",
    "    o1 = result.rfind(\"Option 1\")\n",
    "    o2 = result.rfind(\"Option 2\")\n",
    "    o3 = result.rfind(\"Option 3\")\n",
    "    o4 = result.rfind(\"Option 4\")\n",
    "\n",
    "    answer = np.argmax([o1, o2, o3, o4])\n",
    "    return answer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e074aea86162f5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare LLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc6bcd149b7e3ace"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e27e5606d60547f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f8b47cc346d4291"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_cache=True,\n",
    "        device_map=\"auto\",\n",
    "        max_length=4000,\n",
    "        do_sample=True,\n",
    "        top_k=5,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88fc72bb590a763"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature': 0.0})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62244a66e66adf13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54551fdcee0b6077"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference Utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15a355ddeb652b1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_inference(data, address):\n",
    "    with open(address, 'w') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')\n",
    "            \n",
    "def add_inference(data, address):\n",
    "    with open(address, 'a+') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a45e22965a3d6bfa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execute Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f8e9a165d206798"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "results = []\n",
    "score = 0\n",
    "\n",
    "for que, *options, key in tqdm(itr, total=150, desc=\"Inference (SWAG)\"):\n",
    "    data = {\"question\": que}\n",
    "    for i, opt in enumerate(options, start=1):\n",
    "        data[f\"option_{i}\"] = opt\n",
    "    result = llm_chain.run(data).strip()\n",
    "    data[\"zephyr_raw\"] = result\n",
    "    pred = extract_swag_answer(result)\n",
    "    data[\"zephyr\"] = pred\n",
    "    data[\"answer\"] = key\n",
    "    data[\"score\"] = 1 if pred == key else 0\n",
    "    add_inference([data], OUTPUT_DATA_PATH)\n",
    "    results.append(data)\n",
    "    if pred == key:\n",
    "        score += 1\n",
    "        print(f\"SWAG Score: {round(score/1.5, 3)}%\")\n",
    "        \n",
    "save_inference(results, OUTPUT_BACKUP_PATH)\n",
    "\n",
    "print(f\"Dumped {len(results)} records to {OUTPUT_DATA_PATH}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b37c0d2a3202a564"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
