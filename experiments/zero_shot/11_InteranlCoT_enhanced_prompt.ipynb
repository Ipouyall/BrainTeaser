{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_PATH = \"./SE2024\"\n",
    "INFERENCE_DATASET_PATH = f\"{BASE_PATH}/test_split_data.csv\"\n",
    "\n",
    "OUTPUT_DATA_PATH = f\"{BASE_PATH}/inference_data.jsonl\"\n",
    "OUTPUT_BACKUP_PATH = f\"{BASE_PATH}/inference_data_backup.jsonl\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf3bebb6eef07ac6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(BASE_PATH, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a42ea339b0c6450a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74c6f12665c17c7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def handle_missing_data(data_path, data_id):\n",
    "    if os.path.exists(data_path):\n",
    "        print('Data file already exists')\n",
    "    else:\n",
    "        print(\"Data doesn't exist, start download from the google drive...\")\n",
    "        !gdown $data_id -O $data_path"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e71807284ece3a76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "handle_missing_data(INFERENCE_DATASET_PATH, \"1JcpBjTXv2OfaG6uYcIJO-Yk69nT9uN8i\")\n",
    "inference_data = pd.read_csv(INFERENCE_DATASET_PATH)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d6cebbe24238b29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Template"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97f8993d5587969"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\\n",
    "You are given a riddle and four options to choose the answer amongst them. \\\n",
    "A riddle is a question or statement intentionally phrased so as to require ingenuity in ascertaining its answer or meaning. \\\n",
    "Different ideas can be used in riddles to trick you:\n",
    "    1. Riddles often employ misdirection, leading you away from the actual solution.\n",
    "    2. They include elements with double meanings, requiring a keen eye for words with dual interpretations.\n",
    "    3. Metaphorical wordplay adds another layer, urging you to decipher figurative language.\n",
    "    4. Look out for exaggeration, as riddles may present overly dramatic details to divert your attention.\n",
    "    5. Common phrases and sayings may hide within the puzzle, demanding familiarity.\n",
    "    6. Associations and irony play a crucial role, introducing unexpected connections.\n",
    "    7. Numerical puzzles can also be part of the mystery, requiring you to decode their significance.\n",
    "    8. Elemental imagery, drawn from nature, might hold key descriptors.\n",
    "    9. Rhyming and sound clues can add a poetic dimension.\n",
    "    10. Avoid sexism and sex clich√©, for example, gender bias for jobs, based on their positions or their outcome.\n",
    "    11. Riddle may try to present something impossible or in contradiction with the reality. Just consider alternative perspectives.\n",
    "Although a clever solution is required, avoid supernatural solutions and keep your answer within the limits of realistic imagination. \\\n",
    "For example, having superhuman abilities or unusual events or things are mostly a not preferred choice unless that is a better solution. \\\n",
    "Now which of the following options is the answer to the following riddle:\n",
    "\n",
    "Riddle: \"{riddle}\"\n",
    "\n",
    "Options:\n",
    "Option 1: \"{option_1}\"\n",
    "Option 2: \"{option_2}\"\n",
    "Option 3: \"{option_3}\"\n",
    "Option 4: \"None of the above options are correct\"\n",
    "\n",
    "\n",
    "Let's think step by step. You need to solve the problem in two steps, \\\n",
    "in which in the first step, you consider each option with the riddle individually and thinking if t could be a feasible answer to the riddle; \\\n",
    "In the second step, you need to compare the options, considering context you provided in the previous step and choose the best one as the answer of the riddle, \\\n",
    "and announce the option you think is the best one in the format: 'Option 1' or 'Option 2' or 'Option 3' or 'Option 4':\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(question):\n",
    "    return prompt_template.format(\n",
    "        riddle=question['QUESTION'],\n",
    "        option_1=question['OPTION 1'],\n",
    "        option_2=question['OPTION 2'],\n",
    "        option_3=question['OPTION 3'],\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba97cd3937c89f62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare LLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc6bcd149b7e3ace"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate torch\n",
    "!pip install -q safetensors xformers langchain==0.1.6"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b9097e958a4e749"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e27e5606d60547f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f8b47cc346d4291"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_cache=True,\n",
    "        device_map=\"auto\",\n",
    "        max_length=4000,\n",
    "        do_sample=True,\n",
    "        top_k=5,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88fc72bb590a763"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature': 0.0})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62244a66e66adf13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference Utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15a355ddeb652b1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_inference(data, address):\n",
    "    with open(address, 'w') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')\n",
    "            \n",
    "def add_inference(data, address):\n",
    "    with open(address, 'a+') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a45e22965a3d6bfa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execute Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f8e9a165d206798"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "results = []\n",
    "itr = tqdm(inference_data.iterrows(), total=len(inference_data), desc=\"Processing\")\n",
    "\n",
    "for index, ds in itr:\n",
    "    prompt = get_prompt(ds)\n",
    "    result = llm.invoke(prompt)\n",
    "\n",
    "    data = {\n",
    "        \"question\": ds['QUESTION'],\n",
    "        'option 1': ds['OPTION 1'],\n",
    "        'option 2': ds['OPTION 2'],\n",
    "        'option 3': ds['OPTION 3'],\n",
    "        'option 4': ds['OPTION 4'],\n",
    "        'zephyr': result\n",
    "    }\n",
    "    add_inference([data], OUTPUT_DATA_PATH)\n",
    "    results.append(data)\n",
    "    \n",
    "save_inference(results, OUTPUT_BACKUP_PATH)\n",
    "\n",
    "print(f\"Dumped {len(results)} records to {OUTPUT_DATA_PATH}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b37c0d2a3202a564"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
