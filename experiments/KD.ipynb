{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Utils"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chat Bots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxvrKB6FAwPk"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "\n",
    "class GPTBot:\n",
    "    def __init__(self, model=\"gpt-4\"):\n",
    "        print(\"Initiating GPT chat bot...\")\n",
    "\n",
    "        from secrets import OPENAI_API_KEY\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "        self.model = model\n",
    "        print(\"GPT chat bot Initiated!\")\n",
    "\n",
    "    def get_completion(self, prompt):\n",
    "        while True:\n",
    "            try:\n",
    "                completion = self.__get_completion_handler(prompt)\n",
    "            except:\n",
    "                print(\"GPT completion failed\")\n",
    "                time.sleep(20)\n",
    "            else:\n",
    "                break\n",
    "        return completion\n",
    "\n",
    "    def __get_completion_handler(self, prompt):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0, # this is the degree of randomness of the model's output\n",
    "        )\n",
    "        return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "import datetime\n",
    "\n",
    "class CopyCatBot:\n",
    "    def __init__(self):\n",
    "        print(\"CopyCatBot Initiated!\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_completion(prompt):\n",
    "        pyperclip.copy(prompt)\n",
    "\n",
    "        while True:\n",
    "            completion = pyperclip.paste()\n",
    "            if completion != prompt:\n",
    "                prompt(f\"Completion completed: {datetime.datetime.now()}\")\n",
    "                return completion\n",
    "\n",
    "            pyperclip.copy(prompt)\n",
    "            time.sleep(20)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Puzzle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from enum import Enum, auto\n",
    "\n",
    "class ModeQ(Enum):\n",
    "    Train = auto()\n",
    "    Test = auto()\n",
    "\n",
    "@dataclass()\n",
    "class SentencePuzzle:\n",
    "    id: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    label: int\n",
    "    choices: List[str]\n",
    "    choice_order: List[int]\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.answer is None:\n",
    "            self.mode = ModeQ.Test\n",
    "        else:\n",
    "            self.mode = ModeQ.Train\n",
    "\n",
    "\n",
    "def load_sentence_puzzles(file_path: str) -> list[SentencePuzzle]:\n",
    "    print(f\"Loading sentence puzzles from {file_path}\")\n",
    "    sps = np.load(file_path, allow_pickle=True)\n",
    "    puzzles = [\n",
    "        SentencePuzzle(\n",
    "            id=sp['id'],\n",
    "            question=sp['question'],\n",
    "            answer=sp.get('answer', None),\n",
    "            label=sp.get('label', None),\n",
    "            choices=sp['choice_list'],\n",
    "            choice_order=sp.get('choice_order', None)\n",
    "        )\n",
    "        for sp in sps]\n",
    "    print(f\"Loaded {len(puzzles)} sentence puzzles\")\n",
    "    return puzzles\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Setup"
   ],
   "metadata": {
    "id": "ZOn1CfT8BFQ4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "question = \"A man shaves everyday, yet keeps his beard long.\"\n",
    "answer = \"He wants to maintain his appearance.\"\n",
    "\n",
    "base_prompt = \"\"\"\n",
    "Your task is to generate a descriptive explanation from a question to an answer option. \\\n",
    "In the following, a question and an option as the answer to the question is provided. \\\n",
    "The answer might be or not be a correct answer.\n",
    "\n",
    "Write a descriptive explanation in at most one paragraph and 200 words to show that path from question to the answer.\n",
    "\n",
    "Summarize the review below, delimited by triple\n",
    "backticks, in at most 30 words.\n",
    "\n",
    "Question: ```{question}```\n",
    "Answer OptionL ```{option}```\n",
    "\"\"\"\n",
    "\n",
    "generate_prompt_baseline = lambda que, opt: base_prompt.format(question=que, option=opt)\n",
    "\n",
    "# response = get_completion(prompt)\n",
    "# print(generate_prompt_baseline(question,answer))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQENRkzmA0An",
    "outputId": "3a6fdec3-0f68-48a9-a233-5e9d97d4e5ae"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Demo = True\n",
    "PromptMode = \"baseline\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if Demo:\n",
    "    chat_bot = CopyCatBot()\n",
    "else:\n",
    "    chat_bot = GPTBot()\n",
    "\n",
    "if PromptMode == \"baseline\":\n",
    "    prompt_generator = generate_prompt_baseline\n",
    "else:\n",
    "    print(f\"Unknown prompt generating method: {PromptMode}\")\n",
    "    raise Exception()"
   ],
   "metadata": {
    "id": "_809duiZ_6qU"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
